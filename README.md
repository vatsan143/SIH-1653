# Smart India Hackathon Workshop
# Date: 08-03-2025
## Register Number: 212223230216
## Name: SRIVATSAN G
## Problem Title
SIH 1653: Web based Selector-Applicant Simulation Software
## Problem Description
Background: Recruitment and Assessment Centre (RAC) under DRDO, Ministry of Defence carries out interviews for applications received against advertised vacancies and for promotion to next higher grade for scientific manpower inducted within DRDO. Description: The process of interviewing is a challenging task. An unbiased objective interviewing process helps identify the right talent. The basic process of an interview involves posing a set of questions by an interviewer and thereafter evaluating responses from candidates. Thus, the questions asked should be relevant and match the area/ expertise of the applicant and the responses should also be of relevance w.r.t. the question asked. Expected Solution: The proposed solution should provide experts as well as candidates a real life Board Room experience, starting with initial ice-breaking questions leading to in-depth techno-managerial (depending on the level of candidate) questions. It shall also be able to provide a quantifiable score for experts as well as the candidate for the relevancy of questions w.r.t. the area/ expertise of the applicant. Similarly, candidate responses should also be graded for relevancy w.r.t. the question asked, finally assisting in arriving at an overall score for the subject knowledge of the candidate and thus his/ her suitability against the advertised post.

## Problem Creater's Organization
Ministry of Defence

## Idea
1. Simulated Board Room Experience – The solution should replicate a real-life interview environment, beginning with ice-breaking questions and progressively leading to in-depth technical and managerial discussions based on the candidate’s level.

2. Relevance-Based Questioning – The system should ensure that interview questions are aligned with the candidate’s expertise and the advertised position, eliminating irrelevant or off-topic queries.

3. Automated Response Evaluation – Candidate responses should be analyzed and graded based on their relevance and correctness concerning the posed questions, ensuring an objective assessment.

4. Quantifiable Scoring System – Both interviewers and candidates should receive scores measuring the relevance and appropriateness of questions and responses, contributing to a data-driven selection process.

5. Enhanced Objectivity in Recruitment – By minimizing biases and maintaining a structured assessment, the system will help ensure fair and transparent candidate selection based on merit and expertise.

## Proposed Solution / Architecture Diagram
![WhatsApp Image 2025-03-06 at 18 28 54_93c08f32](https://github.com/user-attachments/assets/76b792e7-cca2-4fea-917d-3b22e98ba3c7)


## Use Cases
![diagram-export-08-03-2025-06_20_16](https://github.com/user-attachments/assets/8817c77d-c207-49f1-96fe-f369d75e13ec)


## Technology Stack

✅ Frontend: React.js / Angular, Tailwind CSS, WebRTC (for real-time interviews)

✅ Backend: Python (Flask/Django) / Node.js (Express), Google Speech-to-Text, OpenAI GPT/BERT

✅ Database: PostgreSQL / MongoDB, Redis (caching)

✅ Cloud & Deployment: AWS / Google Cloud, Docker, Kubernetes, CI/CD (GitHub Actions, Jenkins)

✅ Security: OAuth 2.0 / JWT, AES-256 Encryption, GDPR Compliance

## Dependencies
### Phase	Tasks	Duration

Week 1-2: Planning & Design	Requirement gathering, architecture design, UI/UX wireframes	10-14 days

Week 3-4: Frontend Development	Build candidate & expert interfaces, integrate WebRTC/WebSockets	14 days

Week 5-6: Backend Development	API development (Flask/Django/Node.js), database setup (PostgreSQL/MongoDB)	14 days

Week 7: AI Model Integration	Implement NLP (GPT/BERT), Speech-to-Text, Question Generation	7 days

Week 8: Automated Scoring System	Develop response evaluation & bias mitigation logic	7 days

Week 9: Cloud & Deployment	Docker, Kubernetes setup, deploy to AWS/GCP	7 days

Week 10: Security & Compliance	OAuth 2.0, JWT authentication, GDPR compliance	7 days

Week 11: Testing & Optimization	Unit, integration, and performance testing	7 days

Week 12: Final Deployment & Review	Deployment, final QA, bug fixes, user feedback	7 days

Total Duration: ~12 Weeks (Approx. 84 days)

**Total Estimated Budget**

✅ One-time Development Cost: 75,000 – 170,000

✅ Annual Maintenance & Support: 10,000 – 20,000/year

**Breakdown:**

Development Team: 40,000 – 80,000

AI & NLP Services: 5,000 – 15,000

Cloud Infrastructure: 10,000 – 30,000

Security & Compliance: 5,000 – 10,000

Testing & Optimization: 5,000 – 15,000

